{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import least_squares\n",
    "import mdtraj as md\n",
    "\n",
    "plt.rcParams['axes.linewidth'] = 1\n",
    "plt.rcParams['xtick.major.width'] = 1\n",
    "plt.rcParams['ytick.major.width'] = 1\n",
    "plt.rcParams.update({'font.size': 11})\n",
    "plt.rcParams['contour.negative_linestyle'] = 'dashed'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CCR - the forward model\n",
    "\n",
    "In terms of Bloch-Wangness-Redfield theory, remote CCR rates simply report on the zero spectral density, which is nothing else than the integral of the time correlation function (TCF), i.e., its enclosed area:\n",
    "\\begin{equation}\n",
    "    \\Gamma^{rem}_{\\vec{u},\\vec{v}} \\propto J_{\\vec{u},\\vec{v}}(0) = \\int_0^\\infty C_{\\vec{u},\\vec{v}}(t) \\,dt\n",
    "\\end{equation}\n",
    "\n",
    "$\\vec{u}$ and $\\vec{v}$ represent motional vectors involved in the relaxation process corresponding to dipolar unit vectors or principal components of CSA tensors. The TCF in isotropic solution is:\n",
    "\n",
    "\\begin{equation} \\label{eq:tcf_general}\n",
    "    C_{\\vec{u},\\vec{v}}(t) = \\langle P_2(\\vec{u}(0)\\cdot\\vec{v}(t)) \\rangle\n",
    "\\end{equation}\n",
    "\n",
    "$P_2(x) = 1.5x^2 - 0.5$, angled brackets denote the *ensemble average*. In essence, the TCF encodes the average degree of correlation in $P_2$ between the vectors $\\vec{u}$ and $\\vec{v}$ after a time delay $t$. This might seem a little abstract at first. Let's consider a simple example, the auto-correlated dipolar relaxation of NH spin pairs in the backbone, $\\vec{u} = \\vec{v} = \\vec{NH}$. $C_{\\vec{u},\\vec{u}}(t)$ starts off at $1$: At $t=0$, the NH bonds align with themselves. As time progresses and the protein molecules move in solution, the distribution of projection angles between $\\vec{u}(0)$ and $\\vec{v}(t)$ randomizes throughout the ensemble: $C_{\\vec{u},\\vec{u}}(t)$ decays due to the reorientational motions of the individual NH bonds.\n",
    "\n",
    "We can extend this example by including the contribution of the axially symmetric $^{15}$N CSA tensor. As its unique axis $\\vec{\\sigma_{xx}} = \\vec{v}$ is fixed to the dipolar unit vector $\\vec{NH} = \\vec{u}$ with an angle $\\theta$, any motion experienced by $\\vec{u}$ will also affect $\\vec{v}$. $C_{\\vec{u},\\vec{v}}(t)$ starts off at $\\langle P_2(\\vec{u}(0)\\cdot\\vec{v}(0)) \\rangle = P_2(\\cos\\theta)$ and decays as $t$ progresses. Since $\\theta$ is fixed and relatively small, we can generally approximate:\n",
    "\n",
    "\\begin{equation} \\label{eq:TCF_rigid_isotropic_approximation}\n",
    "C_{\\vec{u},\\vec{v}}(t) \\approx P_2(\\cos\\theta) C_{\\vec{u},\\vec{u}}(t) \\approx P_2(\\cos\\theta) C_{\\vec{v},\\vec{v}}(t)\n",
    "\\end{equation}\n",
    "\n",
    "The same approximation holds true for remote CCR rates if the angle between $\\vec{u}$ and $\\vec{v}$ is fixed. This corresponds to the assumption of a rigid (and isotropically tumbling) fold. Denoting $\\theta$ as a function of the backbone dihedral angles $\\phi$ and/or $\\psi$, we get:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\Gamma^{rem}_{\\vec{u},\\vec{v}} \\propto P_2(\\cos\\theta(\\phi,\\psi))J_{\\vec{u},\\vec{u}}(0)\n",
    "\\end{equation}\n",
    "\n",
    "In the presence of disorder, this gets more complicated. The angular degrees of freedom in the backbone not only govern the decorrelation process itself, they are also reflected in the TCF's amplitude $C_{\\vec{u},\\vec{v}}(0) = \\langle P_2(\\vec{u}(0)\\cdot\\vec{v}(0)) \\rangle = \\langle P_2(\\cos\\theta(\\phi,\\psi)) \\rangle$. Both the amplitude and the decay of the TCF are generally unknown.\n",
    "\n",
    "However, if the dynamics are somewhat comparable between different rates, observed variations should be mostly due to the amplitudes $\\langle P_2(\\cos\\theta(\\phi,\\psi)) \\rangle$. If we can define and extract a reasonable proxy for the normalized TCFs area, i.e, the \"correlation time\", we might be able to combine mutliple CCR rates with different ($\\phi$,$\\psi$)-dependencies to approximate the underlying ($\\phi$,$\\psi$)-distribution. \n",
    "\n",
    "Experimental and preliminary computational studies have shown that this is indeed possible. While improvements and alternatives are still to be investigated, the above approximation based on the auto-correlated $J_{\\vec{NH},\\vec{NH}}(0)$ has proven a simple and adequate correlation time proxy in many scenarios.\n",
    "\n",
    "We will illustrate this process below. First, provide the CCR rates you quantified as well as a $J_{\\vec{NH},\\vec{NH}}(0)$ proxy from the literature provided: [Kadeřávek et al. Journal of biomolecular NMR 63, 353–365 (2015)].\n",
    "\n",
    "We then define our forward models for all six CCR rates $\\Gamma_i(\\phi,\\psi) = k_i\\times P_2(\\cos\\theta(\\phi,\\psi))\\times J_{\\vec{NH},\\vec{NH}}(0)$ building on a predefined backbone geometry. We'll also introduce and define a few extra functions/concepts needed further down below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#provide the residue number i\n",
    "residue_nr = 28\n",
    "\n",
    "#provide your rates in s^-1\n",
    "\n",
    "exp = {}\n",
    "#measured on residue i+1 = \"2\"\n",
    "exp['CaHa1_NHn2'] = -1.00969826949564\n",
    "\n",
    "#measured on residue i = \"1\"\n",
    "exp['CaHa1_NHn1'] = -2.30262939950309\n",
    "\n",
    "#measured on residue i+1 = \"2\"\n",
    "exp['NHn1_NHn2'] = 5.49712296638299\n",
    "\n",
    "#measured on residue i+1 = \"2\"\n",
    "exp['Ha1Hn2_CCSA1'] = 1.25326769676921\n",
    "\n",
    "#measured on residue i+1 = \"2\"\n",
    "exp['CaHa1_CCSA1'] = 6.73446976089051\n",
    "\n",
    "#measured on residue i = \"1\"\n",
    "exp['CaHa1_CCSA0'] = -9.98474886750829\n",
    "\n",
    "#provide the experimental estimate for the auto-correlated NH-NH J(0)\n",
    "#see Kaderavek et al. (2015), Supplementary Information, Table S4, column 3\n",
    "J_0_estimate = 1.389\n",
    "\n",
    "\n",
    "#####################\n",
    "\n",
    "#optional parameters to overwrite\n",
    "\n",
    "#adjust \"order parameter\" if you want to rescale the J_0_estimate\n",
    "dynamic_scaling = 1.0\n",
    "\n",
    "#delete rates you don't want to include in the analysis\n",
    "included_rates = ['CaHa1_NHn2','CaHa1_NHn1','NHn1_NHn2','Ha1Hn2_CCSA1','CaHa1_CCSA1','CaHa1_CCSA0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#provide the residue number i\n",
    "residue_nr = 62\n",
    "\n",
    "#provide your rates in s^-1\n",
    "\n",
    "exp = {}\n",
    "#measured on residue i+1 = \"2\"\n",
    "exp['CaHa1_NHn2'] = -4.29355350079702\n",
    "\n",
    "#measured on residue i = \"1\"\n",
    "exp['CaHa1_NHn1'] = -11.4300983195023\n",
    "\n",
    "#measured on residue i+1 = \"2\"\n",
    "exp['NHn1_NHn2'] = 1.55053181134945\n",
    "\n",
    "#measured on residue i+1 = \"2\"\n",
    "exp['Ha1Hn2_CCSA1'] = 3.78267941005145\n",
    "\n",
    "#measured on residue i+1 = \"2\"\n",
    "exp['CaHa1_CCSA1'] = -9.4569000215266\n",
    "\n",
    "#measured on residue i = \"1\"\n",
    "exp['CaHa1_CCSA0'] = -2.31541020139655\n",
    "\n",
    "#provide the experimental estimate for the auto-correlated NH-NH J(0)\n",
    "#see Kaderavek et al. (2015), Supplementary Information, Table S4, column 3\n",
    "J_0_estimate = 1.045\n",
    "\n",
    "\n",
    "#####################\n",
    "\n",
    "#optional parameters to overwrite\n",
    "\n",
    "#adjust \"order parameter\" if you want to rescale the J_0_estimate\n",
    "dynamic_scaling = 1.0\n",
    "\n",
    "#delete rates you don't want to include in the analysis\n",
    "included_rates = ['CaHa1_NHn2','CaHa1_NHn1','NHn1_NHn2','Ha1Hn2_CCSA1','CaHa1_CCSA1','CaHa1_CCSA0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#provide the residue number i\n",
    "residue_nr = 72\n",
    "\n",
    "#provide your rates in s^-1\n",
    "\n",
    "exp = {}\n",
    "#measured on residue i+1 = \"2\"\n",
    "exp['CaHa1_NHn2'] = -6.15561275346361\n",
    "\n",
    "#measured on residue i = \"1\"\n",
    "exp['CaHa1_NHn1'] = -6.09546407058238\n",
    "\n",
    "#measured on residue i+1 = \"2\"\n",
    "exp['NHn1_NHn2'] = 2.38992810901028\n",
    "\n",
    "#measured on residue i+1 = \"2\"\n",
    "exp['Ha1Hn2_CCSA1'] = 4.55040118134934\n",
    "\n",
    "#measured on residue i+1 = \"2\"\n",
    "exp['CaHa1_CCSA1'] = 0.412801092833432\n",
    "\n",
    "#measured on residue i = \"1\"\n",
    "exp['CaHa1_CCSA0'] = -2.6121008237153\n",
    "\n",
    "#provide the experimental estimate for the auto-correlated NH-NH J(0)\n",
    "#see Kaderavek et al. (2015), Supplementary Information, Table S4, column 3\n",
    "#if the particular residue is not available, interpolate between i-1 and i+1, i.e. (J(0)_{i-1} + J(0)_{i+1})/2\n",
    "#it is provided in ns, jst paste the value here, it will be converted to s later on\n",
    "J_0_estimate = (1.162+0.621)/2\n",
    "\n",
    "\n",
    "#####################\n",
    "\n",
    "#optional parameters to overwrite\n",
    "\n",
    "#adjust \"order parameter\" if you want to rescale the J_0_estimate\n",
    "dynamic_scaling = 1.0\n",
    "\n",
    "#delete rates you don't want to include in the analysis\n",
    "included_rates = ['CaHa1_NHn2','CaHa1_NHn1','NHn1_NHn2','Ha1Hn2_CCSA1','CaHa1_CCSA1','CaHa1_CCSA0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#provide the residue number i\n",
    "residue_nr = 40\n",
    "\n",
    "#provide your rates in s^-1\n",
    "\n",
    "exp = {}\n",
    "#measured on residue i+1 = \"2\"\n",
    "exp['CaHa1_NHn2'] = 5.06367656125063\n",
    "\n",
    "#measured on residue i = \"1\"\n",
    "exp['CaHa1_NHn1'] = -12.9507167961226\n",
    "\n",
    "#measured on residue i+1 = \"2\"\n",
    "exp['NHn1_NHn2'] = -0.114065263297957\n",
    "\n",
    "#measured on residue i+1 = \"2\"\n",
    "exp['Ha1Hn2_CCSA1'] = 1.4310980560296\n",
    "\n",
    "#measured on residue i+1 = \"2\"\n",
    "exp['CaHa1_CCSA1'] = -7.63272676008756\n",
    "\n",
    "#measured on residue i = \"1\"\n",
    "exp['CaHa1_CCSA0'] = -0.041801520814563\n",
    "\n",
    "#provide the experimental estimate for the auto-correlated NH-NH J(0)\n",
    "#see Kaderavek et al. (2015), Supplementary Information, Table S4, column 3\n",
    "#if the particular residue is not available, interpolate between i-1 and i+1, i.e. (J(0)_{i-1} + J(0)_{i+1})/2\n",
    "#it is provided in ns, jst paste the value here, it will be converted to s later on\n",
    "J_0_estimate = 1.235\n",
    "\n",
    "\n",
    "#####################\n",
    "\n",
    "#optional parameters to overwrite\n",
    "\n",
    "#adjust \"order parameter\" if you want to rescale the J_0_estimate\n",
    "dynamic_scaling = 1.0\n",
    "\n",
    "#delete rates you don't want to include in the analysis\n",
    "included_rates = ['CaHa1_NHn2','CaHa1_NHn1','NHn1_NHn2','Ha1Hn2_CCSA1','CaHa1_CCSA1','CaHa1_CCSA0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we need to define some physical constants and experimental paramaters\n",
    "\n",
    "h_bar = 1.05457180013e-34\n",
    "mu_naught = 4.0*np.pi*1.0e-7 \n",
    "gamma_c = 67.262e6 \n",
    "gamma_h = 267.513e6 \n",
    "gamma_n = -27.116e6 \n",
    "\n",
    "B_0 = 800e6*2*np.pi/gamma_h \n",
    "\n",
    "angstrom_correction = 1.0e10**3 #the geometry dependencies are calculated in Angstrom\n",
    "\n",
    "CaHa_pref = mu_naught*h_bar/4/np.pi*gamma_h*gamma_c*angstrom_correction/(1.09**3)\n",
    "NHn_pref = mu_naught*h_bar/4/np.pi*gamma_h*gamma_n*angstrom_correction/(1.02**3)\n",
    "HaHn_pref = mu_naught*h_bar/4/np.pi*gamma_h*gamma_h*angstrom_correction\n",
    "carbonyl_pref = B_0*gamma_c*2/3\n",
    "\n",
    "CC = {} #these will contain the rates as a function of phi/psi\n",
    "sigmas = {} #these will contain the expected variances, we keep it simple and set them to 1\n",
    "E = {} #these will contain the experiments you chose above\n",
    "\n",
    "for rate in included_rates:\n",
    "    interactions = str(rate).split('_')\n",
    "    prefactor = 1\n",
    "    for interaction in interactions:\n",
    "        if 'NHn' in interaction:\n",
    "            prefactor *= NHn_pref\n",
    "        elif 'CaHa' in interaction:\n",
    "            prefactor *= CaHa_pref\n",
    "        elif 'CCSA' in interaction:\n",
    "            prefactor *= carbonyl_pref\n",
    "        elif 'Ha1Hn2' in interaction:\n",
    "            prefactor *= HaHn_pref\n",
    "    #now we import the precalculated geometrical dependencies (assuming a standard backbone geometry)        \n",
    "    #for the correlation time we use the NH-NH autocorrelated J(0) as a proxy\n",
    "    physical_rate = np.loadtxt(str(rate)+'.csv',delimiter=',')*prefactor*J_0_estimate*1e-9*dynamic_scaling\n",
    "    \n",
    "    CC[str(rate)] = physical_rate\n",
    "    #sigmas[str(rate)] = np.amax(physical_rate)-np.amin(physical_rate)\n",
    "    sigmas[str(rate)] = 1.\n",
    "    E[str(rate)] = exp[str(rate)]\n",
    "    \n",
    "#for comparison we use the ensemble of Lange, Griesinger et al. (2008), PDB code 2k39\n",
    "\n",
    "t = md.load('./2k39.pdb') #the ensemble is deposted as a set of 3D structures, we treat it like an MD trajectory\n",
    "\n",
    "phis_t = np.rad2deg(md.compute_phi(t)[1])\n",
    "psis_t = np.rad2deg(md.compute_psi(t)[1])\n",
    "phi_dict = {}\n",
    "psi_dict = {}\n",
    "phi_dict[residue_nr] = [phis_t[x][residue_nr-2] for x in range(len(phis_t))]\n",
    "psi_dict[residue_nr] = [psis_t[x][residue_nr-1] for x in range(len(psis_t))]\n",
    "\n",
    "#our chosen Ramachandran representation is a 360x360 array, which we define below  \n",
    "phi_map = np.empty((360,360))\n",
    "psi_map = np.empty((360,360))\n",
    "\n",
    "for row, psi in enumerate(range(179,-181,-1)):\n",
    "    for column, phi in enumerate(range(-180,180,+1)):\n",
    "        phi_map[row,column] = phi\n",
    "        psi_map[row,column] = psi\n",
    "    \n",
    "#below we define some functions needed for the MaxEnt treatment, no need to understand these!\n",
    "def me_least_squares_normed_reg(L, E, CC, sigmas, tau, scaling, beta):\n",
    "    \n",
    "    summe = sum([L[index]*CC[key]*tau*scaling for index, key in enumerate(E)])\n",
    "    norm = np.amax(summe)\n",
    "    exp_sum = np.exp(summe-norm)\n",
    "    partition = np.sum(prior*exp_sum)\n",
    "    pdf_me = (prior/partition)*exp_sum\n",
    "    \n",
    "    diff = []\n",
    "    for key in E:\n",
    "        #for e in E[key]:\n",
    "        diff.append((np.sum(pdf_me*CC[key]*tau*scaling)-E[key])/sigmas[key])\n",
    "    if beta != 0:\n",
    "        for i, key in enumerate(E):\n",
    "            diff.append(L[i]*beta)\n",
    "            \n",
    "    return np.array(diff)\n",
    "\n",
    "def jacobian_ls_normed_reg(L, E, CC, sigmas, tau, scaling, beta):\n",
    "    \n",
    "    summe = sum([L[index]*CC[key]*tau*scaling for index, key in enumerate(E)])\n",
    "    norm = np.amax(summe)\n",
    "    exp_sum = np.exp(summe-norm)\n",
    "    partition = np.sum(prior*exp_sum)\n",
    "    \n",
    "    G = [CC[key] for key in E]\n",
    "    row_counter = 0\n",
    "    if beta != 0:\n",
    "        jacobian_matrix = np.zeros([(sum([1 for x in E.values()])+len(L)), (len(L))])\n",
    "        #print('jacobian with beta - correct')\n",
    "    else:\n",
    "        jacobian_matrix = np.zeros([(sum([1 for x in E.values()])), (len(L))])    \n",
    "    for key in E:\n",
    "        #for row_entry in range(len(E[key])):\n",
    "        for column in range(len(L)):\n",
    "            jacobian_matrix[row_counter,column] = 1/sigmas[key]*(np.sum(prior*CC[key]*tau*scaling*G[column]*tau*scaling*exp_sum)/partition -\n",
    "                                                            (np.sum(prior*CC[key]*tau*scaling*exp_sum)*np.sum(prior*G[column]*tau*scaling*exp_sum))/(partition**2))\n",
    "        row_counter += 1\n",
    "        \n",
    "    if beta !=0:\n",
    "        for x in range(len(L)):\n",
    "            for column in range(len(L)):\n",
    "                if x == column:\n",
    "                    jacobian_matrix[row_counter,column] = beta\n",
    "                else:\n",
    "                    jacobian_matrix[row_counter,column] = 0.\n",
    "            \n",
    "            row_counter += 1\n",
    "                   \n",
    "    return jacobian_matrix\n",
    "\n",
    "#finally, let's plot the expected geometrical dependencies P2(cos(theta))\n",
    "figure_width = 16\n",
    "\n",
    "fig, axs = plt.subplots(1,6,figsize=(figure_width,figure_width/6.6))\n",
    "for i, key in enumerate(CC):\n",
    "    x = CC[key]\n",
    "\n",
    "    if np.amin(x) < -np.amax(x):\n",
    "        elev_max = -np.amin(x)\n",
    "        elev_min = np.amin(x)\n",
    "    else:\n",
    "        elev_max = np.amax(x)\n",
    "        elev_min = -np.amax(x)\n",
    "\n",
    "    levels = np.linspace(elev_min,elev_max,11)\n",
    "    \n",
    "    axs[i].contourf(np.flipud(x), levels, cmap=plt.cm.RdBu_r)\n",
    "    axs[i].set_xticks([0,359])\n",
    "    axs[i].set_xticklabels(['-180°','180°'])\n",
    "    axs[i].set_yticks([0,359])\n",
    "    if i == 0:\n",
    "        axs[i].set_yticklabels(['-180°','180°'])\n",
    "        axs[i].set_ylabel('$\\mathregular{\\psi}$')\n",
    "    else:\n",
    "        axs[i].set_yticklabels([])\n",
    "    axs[i].set_xlabel('$\\mathregular{\\phi}$')\n",
    "    axs[i].xaxis.labelpad = -8\n",
    "    axs[i].yaxis.labelpad = -20\n",
    "    axs[i].contour(np.flipud(x), levels, linewidths = .5, colors = 'k')\n",
    "    axs[i].set_title(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes' theorem - A crashcourse\n",
    "\n",
    "You have measured and quantified a set of 6 CCR rates $\\Gamma_i^{exp}$, $i=1,\\ldots,6$ or, in shorthand vectorial notation, $\\vec{\\Gamma}^{exp}$. We have also defined our forward models $\\Gamma_i(\\phi,\\psi)$ which relate the TCF amplitudes to the encoded $(\\phi,\\psi)$-space.\n",
    "\n",
    "Assuming we have a (somewhat) rigid backbone, we now may ask: What $(\\phi,\\psi)$-pair would best explain our observations?\n",
    "\n",
    "In terms of conditional probability, this translates to: What probability do we assign a $(\\phi,\\psi)$-pair *given* our observations, i.e., $P\\big((\\phi,\\psi)\\vert\\vec{\\Gamma}^{exp}\\big)$? This is simply the intersection, denoted \"$\\cap$\", of possibilities: With the observations $\\vec{\\Gamma}^{exp}$ at hand, how likely are they due to $(\\phi,\\psi)$? This is the defintion of conditional probability:\n",
    "\n",
    "\\begin{align}\n",
    "    P\\big((\\phi,\\psi)\\vert\\vec{\\Gamma}^{exp}\\big) &= \\frac{P\\big((\\phi,\\psi)\\cap\\vec{\\Gamma}^{exp}\\big)}{P\\big(\\vec{\\Gamma}^{exp}\\big)}\n",
    "\\end{align}\n",
    "\n",
    "Bayes' theorem is simply a way of rearranging this expression. We multiply by $P\\big((\\phi,\\psi)\\big) / P\\big((\\phi,\\psi)\\big) = 1$ and exploit the fact that the intersection does not depend on the order, $P\\big((\\phi,\\psi)\\cap\\vec{\\Gamma}^{exp}\\big) = P\\big(\\vec{\\Gamma}^{exp}\\cap(\\phi,\\psi)\\big)$, which yields:\n",
    "\n",
    "\\begin{align}\n",
    "    P\\big((\\phi,\\psi)\\vert\\vec{\\Gamma}^{exp}\\big) &= \\frac{P\\big((\\phi,\\psi)\\cap\\vec{\\Gamma}^{exp}\\big)}{P\\big(\\vec{\\Gamma}^{exp}\\big)} = \\frac{\\dfrac{P\\big((\\phi,\\psi)\\cap\\vec{\\Gamma}^{exp}\\big)}{P\\big((\\phi,\\psi)\\big)}P\\big((\\phi,\\psi)\\big)}{P\\big(\\vec{\\Gamma}^{exp}\\big)}\\\\\n",
    "    &= \\frac{P\\big(\\vec{\\Gamma}^{exp}\\vert(\\phi,\\psi)\\big)P\\big((\\phi,\\psi)\\big)}{P\\big(\\vec{\\Gamma}^{exp}\\big)}\n",
    "\\end{align}\n",
    "\n",
    "The probability $P\\big(\\vec{\\Gamma}^{exp}\\big)$ does not depend on $(\\phi,\\psi)$. Since we are only interested in comparing different realizations of $(\\phi,\\psi)$, $P\\big(\\vec{\\Gamma}^{exp}\\big)$ enters as a constant, so we can simplify:\n",
    "\n",
    "\\begin{align}\n",
    "    P\\big((\\phi,\\psi)\\vert\\vec{\\Gamma}^{exp}\\big) \\propto P\\big(\\vec{\\Gamma}^{exp}\\vert(\\phi,\\psi)\\big)P\\big((\\phi,\\psi)\\big)\n",
    "\\end{align}\n",
    "\n",
    "In Bayesian terminology, this reads: The *Posterior* is proportional to the *Likelihood* times the *Prior*. We don't know the quantity of interest, the Posterior. But we are fully equipped to define both the Likelihood and the Prior.\n",
    "\n",
    "## A Likelihood estimator\n",
    "\n",
    "Let's assume our data is very informative, i.e., sufficient to single out a $(\\phi,\\psi)$-pair from a very general set of possible values: All angles are equally likely, the prior $P\\big((\\phi,\\psi)\\big)$ is uniform, i.e., a constant. Then our posterior probability $P\\big((\\phi,\\psi)\\vert\\vec{\\Gamma}^{exp}\\big)$ reduces to the Likelihood.\n",
    "\n",
    "The Likelihood connects the observed rates $\\Gamma_i^{exp}$ with our forward models $\\Gamma_i(\\phi,\\psi)$. A common assumption is a *Gaussian* Likelihood, which assumes a set of uncorrelated, independent measurements with Gaussian noise, which yields a product of Gaussians written either as a product of exponentials or, more conveniently, as a sum in the exponent:\n",
    "\n",
    "\\begin{align}\n",
    "    P\\big((\\phi,\\psi)\\vert\\vec{\\Gamma}^{exp}\\big) &\\propto \\exp \\bigg(-\\frac{\\big(\\Gamma_1^{exp}-\\Gamma_1(\\phi,\\psi)\\big)^2}{\\sigma_1^2}\\bigg) \\times \\exp \\bigg(-\\frac{\\big(\\Gamma_2^{exp}-\\Gamma_2(\\phi,\\psi)\\big)^2}{\\sigma_2^2}\\bigg) \\times \\ldots\\\\\n",
    "    &\\propto \\exp \\bigg(-\\sum_{i=1}^6\\frac{\\big(\\Gamma_i^{exp}-\\Gamma_i(\\phi,\\psi)\\big)^2}{\\sigma_i^2}\\bigg)\n",
    "\\end{align}\n",
    "\n",
    "In essence, $(\\phi,\\psi)$-pairs with low squared residuals are considered more likely. We simplify by setting the variances $\\sigma_i^2 = 1$ for all rates. The results are calculated below. For visualization purposes, let's look at the Gaussians individually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure_width = 16\n",
    "\n",
    "Z_surface = {}\n",
    "\n",
    "Z_product = 1.\n",
    "Z_sum = 1.\n",
    "\n",
    "fig, axs = plt.subplots(1,6,figsize=(figure_width,figure_width/6.6))\n",
    "\n",
    "for i, key in enumerate(CC):\n",
    "    Z_surface[key] = np.exp(-np.square(CC[key]-E[key])/sigmas[key])\n",
    "    axs[i].set_xticks([0,359])\n",
    "    axs[i].set_yticks([0,359])\n",
    "    axs[i].set_title('L('+key+')')\n",
    "    axs[i].set_xticklabels(['-180°','180°'])\n",
    "    axs[i].set_xticklabels(['-180°','180°'])\n",
    "    axs[i].set_xlabel('$\\mathregular{\\phi}$')\n",
    "    axs[i].xaxis.labelpad = -8\n",
    "    \n",
    "    if i == 0:\n",
    "        axs[i].set_yticklabels(['-180°','180°'])\n",
    "        axs[i].set_ylabel('$\\mathregular{\\psi}$')\n",
    "\n",
    "        axs[i].yaxis.labelpad = -20\n",
    "    else:\n",
    "        axs[i].set_yticklabels([])\n",
    "    \n",
    "    axs[i].imshow(Z_surface[key])    \n",
    "    \n",
    "    Z_product *= Z_surface[key]\n",
    "    Z_sum += Z_surface[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rates each constrain the space of possible $(\\phi,\\psi)$-values differently. Note how the $(\\phi,\\psi)$-dependencies from above determine are mirrored in the shape of the Gaussians.\n",
    "\n",
    "We are interested in the total Likelihood, i.e., the product of these Gaussians. For visualization purposes, we may also consider their overlap to provide a better \"feel\". How does it compare to the PDB ensemble of Lange et al.?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure_width = 13\n",
    "\n",
    "fig, axs = plt.subplots(1,3,figsize=(figure_width,figure_width/3.3))\n",
    "\n",
    "x1 = np.ndarray.flatten(phi_map)\n",
    "y1 = np.ndarray.flatten(psi_map)\n",
    "C1 = np.ndarray.flatten(Z_product)\n",
    "grid = 10\n",
    "\n",
    "axs[1].axis([-180,180,-180,180])\n",
    "axs[1].set_xticks([-180,180])\n",
    "axs[1].set_yticks([-180,180])\n",
    "axs[1].set_xticklabels(['-180°','180°'])\n",
    "axs[1].set_yticklabels([])\n",
    "axs[1].set_xlabel('$\\mathregular{\\phi}$')\n",
    "#axs[1].set_ylabel('$\\mathregular{\\psi}$')\n",
    "axs[1].set_title('Likelihood Fit')\n",
    "axs[1].hexbin(x1,y1,C1,gridsize=(grid,grid),cmap='bone_r',extent=[-180,180,-180,180])\n",
    "#axs[1].xaxis.labelpad = -8\n",
    "\n",
    "axs[0].set_xticks([0,359])\n",
    "axs[0].set_yticks([0,359])\n",
    "axs[0].set_xticklabels(['-180°','180°'])\n",
    "axs[0].set_yticklabels(['-180°','180°'])\n",
    "axs[0].set_xlabel('$\\mathregular{\\phi}$')\n",
    "axs[0].set_ylabel('$\\mathregular{\\psi}$')\n",
    "axs[0].set_title('Gaussian Overlay')\n",
    "axs[0].xaxis.labelpad = -8\n",
    "axs[0].yaxis.labelpad = -20\n",
    "axs[0].imshow(Z_sum)\n",
    "\n",
    "x2 = phi_dict[residue_nr]\n",
    "y2 = psi_dict[residue_nr]\n",
    "\n",
    "axs[2].axis([-180,180,-180,180])\n",
    "axs[2].set_xticks([-180,180])\n",
    "axs[2].set_yticks([-180,180])\n",
    "axs[2].set_xticklabels(['-180°','180°'])\n",
    "axs[2].set_yticklabels([])\n",
    "axs[2].set_xlabel('$\\mathregular{\\phi}$')\n",
    "#axs[1].set_ylabel('$\\mathregular{\\psi}$')\n",
    "axs[2].set_title('PDB ensemble')\n",
    "axs[2].hexbin(x2,y2,gridsize=(grid,grid),cmap='bone_r',extent=[-180,180,-180,180])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way we constructed our Likelihood, we tend to find very sharp and localized peaks centered at the most probable realization of $(\\phi,\\psi)$. This is the case even if the underlying ensemble might exhibit some heterogeneity. Assuming a sharp unimodal solution, the CCR rates appear to constrain the $(\\phi,\\psi)$-space sufficiently. We could as well look for the maximum of our Posterior directly, which, in this case, is the maximum of the Likelihood. The above procedure therefore mimics a so-called Maximum Likelihood estimator. For a Gaussian Likelihood, this corresponds to a very common and probably familiar fitting procedure:\n",
    "\n",
    "\\begin{align}\n",
    "    \\max_{\\phi,\\psi} P\\big((\\phi,\\psi)\\vert\\vec{\\Gamma}^{exp}\\big) &= \\max_{\\phi,\\psi} \\log P\\big((\\phi,\\psi)\\vert\\vec{\\Gamma}^{exp}\\big) = \\min_{\\phi,\\psi} -\\log P\\big((\\phi,\\psi)\\vert\\vec{\\Gamma}^{exp}\\big)\\\\\n",
    "    &= \\min_{\\phi,\\psi} \\sum_{i=1}^6\\frac{\\big(\\Gamma_i^{exp}-\\Gamma_i(\\phi,\\psi)\\big)^2}{\\sigma_i^2} := \\min_{\\phi,\\psi} \\chi^2(\\phi,\\psi)\n",
    "\\end{align}\n",
    "\n",
    "In essence, we did a simple $\\chi^2$-fit, minimizing the squared distance between measured and calculated rates - assuming a rigid fold!\n",
    "\n",
    "## Maximum A Posteriori (MAP)\n",
    "\n",
    "By assuming a sharp peak in $(\\phi,\\psi)$-space, we drastically simplified the space of possible solutions; the Prior was assumed to be sparse. Clearly, this isn't always the case. If we want to allow for actual $(\\phi,\\psi)$-distributions $\\vec{p}_{\\phi,\\psi}$, we must rewrite Bayes' theorem accordingly:\n",
    "\n",
    "\\begin{align}\n",
    "    P\\big(\\vec{p}_{\\phi,\\psi}\\vert\\vec{\\Gamma}^{exp}\\big) \\propto P\\big(\\vec{\\Gamma}^{exp}\\vert\\vec{p}_{\\phi,\\psi}\\big)P\\big(\\vec{p}_{\\phi,\\psi}\\big)\n",
    "\\end{align}\n",
    "\n",
    "Due to the ambiguities that come with pronounced *ensemble averaging*, we should not presume that the Likelihood can always carry the Posterior's mass alone. The less informative the Likelihood *could* be, the more specific the Prior should be constructed.\n",
    "\n",
    "Without showing a detailed derivation, we construct a similar point estimate from the Posterior, this time treating the Prior explicitly in a so-called *Maximum A Posteriori* (MAP) estimate:\n",
    "\\begin{align}\n",
    "    min_{\\vec{p}_{\\phi,\\psi}} P\\big(\\vec{p}_{\\phi,\\psi}\\big) + \\sum_{i=1}^6\\frac{\\big(\\Gamma_i^{exp}-\\langle\\Gamma_i(\\phi,\\psi)\\rangle\\big)^2}{\\sigma_i^2}\n",
    "\\end{align}\n",
    "\n",
    "Again we assume a Gaussian Likelihood but we consider the forward model in terms of $\\langle \\Gamma_i(\\phi,\\psi) \\rangle$, a population average over the $(\\phi,\\psi)$-distribution $\\vec{p}_{\\phi,\\psi}$. The Prior $P\\big(\\vec{p}_{\\phi,\\psi}\\big)$ contains an entropy measure that penalizes solutions with large realtive entropy / Kullback-Leibler-divergence with respect to a random coil distribution calculated from the PDB. For more details, see [Kauffmann, Zawadzka-Kazimierczuk, Kontaxis, Konrat. ChemPhysChem 22, 18–28 (2021)].\n",
    "\n",
    "Again, if we decide to only characterize the maximum, we obtain a \"simple\" regularized $\\chi^2$-fit. The balance between Prior and Likelihood enters as an adjustable paramater which we will fix preliminarily. Let's calculate and compare the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure_width = 13\n",
    "\n",
    "prior = np.loadtxt('prior.csv', delimiter=',')/np.sum(np.loadtxt('prior.csv', delimiter=','))\n",
    "ls_routine_keywords = {'verbose': 2, 'method': 'lm', 'x_scale': 'jac'}\n",
    "\n",
    "L = np.zeros(len(E))\n",
    "minimization = least_squares(me_least_squares_normed_reg, L, args=(E, CC, sigmas, 1, 1, 2.5), jac=jacobian_ls_normed_reg, **ls_routine_keywords)\n",
    "\n",
    "summe = sum([minimization['x'][index]*CC[key] for index, key in enumerate(E)])\n",
    "norm = np.amax(summe)\n",
    "\n",
    "exp_sum = np.exp(summe-norm)\n",
    "partition = np.sum(prior*exp_sum)\n",
    "pdf_me = (prior/partition)*exp_sum\n",
    "\n",
    "fig, axs = plt.subplots(1,3,figsize=(figure_width,figure_width/3.3))\n",
    "x = np.ndarray.flatten(phi_map)\n",
    "y = np.ndarray.flatten(psi_map)\n",
    "C = np.ndarray.flatten(pdf_me)\n",
    "\n",
    "axs[1].axis([-180,180,-180,180])\n",
    "axs[1].set_xticks([-180,180])\n",
    "axs[1].set_yticks([-180,180])\n",
    "axs[1].set_xticklabels(['-180°','180°'])\n",
    "axs[1].set_yticklabels([])\n",
    "axs[1].set_xlabel('$\\mathregular{\\phi}$')\n",
    "#axs[1].set_ylabel('$\\mathregular{\\psi}$')\n",
    "axs[1].set_title('Maximum Entropy')\n",
    "\n",
    "grid = 10\n",
    "axs[1].hexbin(x,y,C,gridsize=(grid,grid),cmap='bone_r')\n",
    "\n",
    "x2 = phi_dict[residue_nr]\n",
    "y2 = psi_dict[residue_nr]\n",
    "\n",
    "#basins = [str(round(basins_olga(np.flipud(np.array(data[str(res)]['R'])))[x],2)) for x,i in enumerate(basins_olga(res_dict[res]['rama_plot']))]\n",
    "\n",
    "axs[0].axis([-180,180,-180,180])\n",
    "axs[0].set_xticks([-180,180])\n",
    "axs[0].set_yticks([-180,180])\n",
    "axs[0].set_xticklabels(['-180°','180°'])\n",
    "axs[0].set_yticklabels(['-180°','180°'])\n",
    "axs[0].set_xlabel('$\\mathregular{\\phi}$')\n",
    "axs[0].set_ylabel('$\\mathregular{\\psi}$')\n",
    "axs[0].set_title('PDB')\n",
    "axs[0].hexbin(x2,y2,gridsize=(grid,grid),cmap='bone_r',extent=[-180,180,-180,180])\n",
    "\n",
    "x3 = np.ndarray.flatten(phi_map)\n",
    "y3 = np.ndarray.flatten(psi_map)\n",
    "C3 = np.ndarray.flatten(Z_product)\n",
    "\n",
    "axs[2].axis([-180,180,-180,180])\n",
    "axs[2].set_xticks([-180,180])\n",
    "axs[2].set_yticks([-180,180])\n",
    "axs[2].set_xticklabels(['-180°','180°'])\n",
    "axs[2].set_yticklabels([])\n",
    "axs[2].set_xlabel('$\\mathregular{\\phi}$')\n",
    "#axs[1].set_ylabel('$\\mathregular{\\psi}$')\n",
    "axs[2].set_title('Sparse Likelihood Fit')\n",
    "axs[2].hexbin(x3,y3,C3,gridsize=(grid,grid),cmap='bone_r',extent=[-180,180,-180,180])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignments and optional(!) questions\n",
    "\n",
    "A 1) Provide the CCR rates extracted for residue 28, 62 and 72.\n",
    "\n",
    "A 2) Provide the comparison of ($\\phi$,$\\psi$)-distributions obtained from the PDB ensemble 2k39 and both MaxEnt and Sparse Likelihhod fits for residues 28, 62, 72.\n",
    "\n",
    "Q 1) The MaxEnt approach builds on the assumption of a \"random coil\". The Sparse Likelihood approach assumes a rigid fold. Why do you think MaxEnt can fit both rigid and flexible residues while the Sparse Likelihood cannot?\n",
    "\n",
    "Q 2) A lot of approximations have been introduced above, both in the forward model and in the statistical treatment. Experimental uncertainties were also not accounted for explicitly. Why do you think the approach still works reasonably well? And what aspects could be improved upon to yield better results?\n",
    "\n",
    "Q 3) In order to extract the structural information contained in the TCF's amplitude, we had to assume a correlation time. Already our crude proxy from the literature appeared to yield reasonable results. Why do you think that is? Does it have to be very exact? (You can try it out, if you want!) If you had to improve on the correlation time proxy, what other experiments can you think of?\n",
    "\n",
    "Q 4) By extracting the ($\\phi$,$\\psi$)-distributions from different CCR rates, we can also extract their correlation times. Thus, the spectral densities could be mapped in purely dynamical terms and compared with other (simpler) relaxation paramaters. How does this separation process of structure and dynamics work formally?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
